# NKAT 電源断リカバリーシステム マニュアル

## 概要

NKAT電源断リカバリーシステムは、RTX3080 CUDA環境での機械学習トレーニング中における予期しない電源断やシステム障害からの自動復旧を実現するシステムです。

## 🔋 主要機能

### 1. 自動チェックポイント機能
- **定期保存**: 設定した間隔（デフォルト5分）で自動保存
- **最良モデル保存**: 精度向上時に最良モデルを自動バックアップ
- **アトミック保存**: 一時ファイル経由で破損リスク最小化

### 2. 緊急保存システム
- **シグナルハンドラー**: Ctrl+C、SIGTERM での緊急保存
- **例外処理**: 訓練エラー時の自動状態保存
- **メモリ状態記録**: CUDA/システムメモリ使用量の記録

### 3. 自動復旧機能
- **セッション復元**: 中断したセッションからの自動再開
- **状態整合性**: モデル、オプティマイザー、スケジューラーの完全復元
- **デバイス互換性**: GPU/CPU間での柔軟な復旧

## 📦 システム構成

```
nkat_power_recovery_system.py     # メインリカバリーシステム
nkat_cifar10_recovery_training.py # CIFAR-10統合トレーニング
test_recovery_system.py           # システムテスト
```

## 🚀 基本使用方法

### 1. システム初期化

```python
from nkat_power_recovery_system import NKATPowerRecoverySystem

# リカバリーシステム初期化
recovery_system = NKATPowerRecoverySystem(
    checkpoint_dir="checkpoints",      # チェックポイント保存ディレクトリ
    backup_dir="backups",             # バックアップディレクトリ
    auto_save_interval=300            # 自動保存間隔（秒）
)
```

### 2. CIFAR-10トレーニング実行

```bash
# 基本実行（50エポック）
py -3 nkat_cifar10_recovery_training.py

# エポック数指定実行
py -3 nkat_cifar10_recovery_training.py 100

# バックグラウンド実行
py -3 nkat_cifar10_recovery_training.py 200 &
```

### 3. 中断からの復旧

システムが中断した場合、同じコマンドを再実行するだけで自動復旧します：

```bash
# 中断後の再実行で自動復旧
py -3 nkat_cifar10_recovery_training.py 100
```

## 🔧 高度な使用方法

### 1. 手動チェックポイント保存

```python
checkpoint_path = recovery_system.save_checkpoint(
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    epoch=current_epoch,
    best_accuracy=best_acc,
    loss=current_loss,
    metrics=training_metrics,
    checkpoint_type="manual"  # "auto", "best", "manual", "emergency"
)
```

### 2. 特定チェックポイントからの復旧

```python
# 最新チェックポイントからの復旧
checkpoint_data = recovery_system.load_checkpoint()

# 特定ファイルからの復旧
checkpoint_data = recovery_system.load_checkpoint("path/to/checkpoint.pth")

# モデル状態復元
if checkpoint_data:
    model.load_state_dict(checkpoint_data['model_state_dict'])
    optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint_data['scheduler_state_dict'])
```

### 3. 緊急保存

```python
# 手動緊急保存
recovery_system.force_checkpoint("manual_emergency")

# システム情報付き緊急保存
recovery_system.force_checkpoint("high_memory_usage")
```

## 📊 監視とレポート

### 1. リアルタイムレポート生成

```python
# システム状態レポート
report = recovery_system.generate_recovery_report()
```

### 2. 進捗監視

```bash
# 連続監視モード
py -3 monitor_cifar.py --continuous

# W&B統合監視
py -3 monitor_cifar.py --wandb
```

## ⚡ 電源断対応ベストプラクティス

### 1. 設定推奨値

```python
# 高頻度保存（重要なトレーニング）
auto_save_interval=180  # 3分間隔

# 標準設定
auto_save_interval=300  # 5分間隔

# 軽量設定（長時間トレーニング）
auto_save_interval=600  # 10分間隔
```

### 2. ストレージ管理

- **SSD推奨**: チェックポイント保存にはSSDを使用
- **定期クリーンアップ**: 古いチェックポイントは自動削除（最新5個保持）
- **バックアップ**: 最良モデルは別ディレクトリにバックアップ

### 3. メモリ管理

```python
# 大容量モデルの場合
if torch.cuda.memory_allocated() / 1024**3 > 8:  # 8GB超過
    torch.cuda.empty_cache()
    recovery_system.force_checkpoint("high_memory")
```

## 🛠️ トラブルシューティング

### 1. よくある問題と解決方法

#### 問題: チェックポイント読み込みエラー

```
❌ Checkpoint load failed: RuntimeError: Error(s) in loading state_dict
```

**解決方法:**
1. 破損したチェックポイントファイルを削除
2. バックアップディレクトリから復旧
3. 新しいセッションで再開

#### 問題: CUDA メモリ不足

```
❌ CUDA out of memory
```

**解決方法:**
```python
# メモリクリア後再試行
torch.cuda.empty_cache()
recovery_system.force_checkpoint("memory_cleanup")
# バッチサイズを削減して再実行
```

#### 問題: デバイス不一致

```
⚠️ Device mismatch: NVIDIA GeForce RTX 3080 -> cpu
```

**解決方法:**
- GPU利用可能な環境で実行
- CPUでの復旧も自動対応

### 2. ログ分析

重要なログメッセージの意味：

```
🔋 Power Recovery System initialized     # システム正常初期化
💾 Checkpoint saved                      # チェックポイント保存成功
🔄 Loading checkpoint                    # 復旧開始
✅ Checkpoint loaded successfully        # 復旧成功
🚨 Emergency save triggered              # 緊急保存実行
🆘 Emergency state saved                 # 緊急状態保存完了
```

## 📈 パフォーマンス監視

### 1. GPU使用量監視

```python
# メモリ使用量チェック
memory_used = torch.cuda.memory_allocated() / 1024**3
if memory_used > 9:  # RTX3080の90%使用時
    print(f"⚠️ High GPU memory: {memory_used:.1f}GB")
```

### 2. 自動保存効率

- **保存時間**: 通常1-3秒以内
- **ファイルサイズ**: モデルサイズに依存（通常100-500MB）
- **I/O負荷**: SSD使用で最小化

### 3. 復旧時間

- **自動検出**: 1秒以内
- **チェックポイント読み込み**: 3-5秒
- **状態復元**: 1-2秒

## 🔒 セキュリティ考慮事項

### 1. チェックポイントファイル

- **アクセス権限**: 適切なファイル権限設定
- **暗号化**: 機密データの場合は暗号化推奨
- **バックアップ**: 重要モデルは外部ストレージにバックアップ

### 2. セッション管理

- **セッションID**: ユニークなIDで識別
- **タイムスタンプ**: 正確な時刻記録
- **メタデータ**: 完全な復旧情報保持

## 📝 カスタマイズ例

### 1. カスタムメトリクス保存

```python
custom_metrics = {
    'learning_rate': current_lr,
    'gradient_norm': grad_norm,
    'validation_f1': f1_score,
    'custom_loss': my_loss
}

recovery_system.save_checkpoint(
    model, optimizer, scheduler, epoch,
    best_accuracy, loss, custom_metrics, "custom"
)
```

### 2. 条件付き保存

```python
# 精度改善時のみ保存
if current_accuracy > best_accuracy + 0.5:  # 0.5%改善時
    recovery_system.save_checkpoint(
        model, optimizer, scheduler, epoch,
        current_accuracy, loss, metrics, "improvement"
    )
```

### 3. 外部通知統合

```python
import smtplib

def send_recovery_notification(recovery_count):
    if recovery_count > 0:
        # メール通知
        send_email(f"Training recovered {recovery_count} times")
        
        # Slack通知
        send_slack_message(f"🔄 NKAT training recovered")
```

## 🎯 運用チェックリスト

### 開始前確認
- [ ] CUDA環境確認 (`nvidia-smi`)
- [ ] 十分なストレージ容量
- [ ] チェックポイントディレクトリの書き込み権限
- [ ] tqdm、psutilライブラリ導入済み

### 実行中監視
- [ ] GPU温度・使用率監視
- [ ] チェックポイント定期確認
- [ ] ログ出力正常確認
- [ ] ストレージ使用量監視

### 終了後確認
- [ ] 最終チェックポイント保存確認
- [ ] 最良モデルバックアップ確認
- [ ] 訓練グラフ生成・保存
- [ ] システムレポート確認

---

## 📞 サポート

システムに関する問題や改善提案は、プロジェクトのIssueトラッカーまでお寄せください。

**重要**: 本リカバリーシステムは、ハードウェア障害、ネットワーク障害、電源断などの予期しない中断からの復旧を支援しますが、完全な障害耐性を保証するものではありません。重要なトレーニングでは定期的な手動バックアップも併用することを推奨します。 