#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT Power Recovery System
RTX3080 CUDAå¯¾å¿œ é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import torch
import pickle
import shutil
import psutil
import hashlib
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from tqdm import tqdm
import time

class NKATPowerRecoverySystem:
    """é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 checkpoint_dir: str = "checkpoints",
                 backup_dir: str = "backups", 
                 auto_save_interval: int = 300):  # 5åˆ†é–“éš”
        
        self.checkpoint_dir = Path(checkpoint_dir)
        self.backup_dir = Path(backup_dir)
        self.auto_save_interval = auto_save_interval
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)
        
        self.recovery_metadata = {
            'session_id': self.generate_session_id(),
            'start_time': datetime.now().isoformat(),
            'cuda_device': None,
            'last_checkpoint': None,
            'recovery_count': 0
        }
        
        self.initialize_cuda()
        self.setup_recovery_hooks()
        
        print(f"ğŸ”‹ Power Recovery System initialized")
        print(f"   Session ID: {self.recovery_metadata['session_id']}")
        print(f"   CUDA Device: {self.recovery_metadata['cuda_device']}")
        print(f"   Checkpoint Dir: {self.checkpoint_dir}")
        
    def initialize_cuda(self):
        """CUDAç’°å¢ƒåˆæœŸåŒ–"""
        if torch.cuda.is_available():
            device_id = 0  # RTX3080
            torch.cuda.set_device(device_id)
            device_name = torch.cuda.get_device_name(device_id)
            self.recovery_metadata['cuda_device'] = device_name
            
            # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            torch.cuda.empty_cache()
            
            print(f"âœ… CUDA initialized: {device_name}")
            print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        else:
            print("âš ï¸ CUDA not available")
            
    def generate_session_id(self) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
        return f"nkat_{timestamp}_{unique_id}"
    
    def setup_recovery_hooks(self):
        """ãƒªã‚«ãƒãƒªãƒ¼ãƒ•ãƒƒã‚¯è¨­å®š"""
        import signal
        import atexit
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        signal.signal(signal.SIGINT, self.emergency_save_handler)
        signal.signal(signal.SIGTERM, self.emergency_save_handler)
        
        # çµ‚äº†æ™‚å‡¦ç†
        atexit.register(self.cleanup_on_exit)
        
    def emergency_save_handler(self, signum, frame):
        """ç·Šæ€¥ä¿å­˜ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
        print(f"\nğŸš¨ Emergency save triggered (signal {signum})")
        self.force_checkpoint("emergency_interrupt")
        exit(0)
        
    def cleanup_on_exit(self):
        """çµ‚äº†æ™‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("\nğŸ§¹ Cleaning up CUDA memory...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    def save_checkpoint(self, 
                       model: torch.nn.Module,
                       optimizer: torch.optim.Optimizer,
                       scheduler: Optional[Any],
                       epoch: int,
                       best_accuracy: float,
                       loss: float,
                       metrics: Dict[str, Any],
                       checkpoint_type: str = "auto") -> str:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        
        checkpoint_data = {
            'session_id': self.recovery_metadata['session_id'],
            'timestamp': datetime.now().isoformat(),
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'best_accuracy': best_accuracy,
            'current_loss': loss,
            'metrics': metrics,
            'cuda_device': self.recovery_metadata['cuda_device'],
            'checkpoint_type': checkpoint_type,
            'recovery_count': self.recovery_metadata['recovery_count']
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        checkpoint_name = f"nkat_checkpoint_{checkpoint_type}_epoch{epoch:03d}_{timestamp}.pth"
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        
        # ä¿å­˜å®Ÿè¡Œ
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜å¾Œã€ã‚¢ãƒˆãƒŸãƒƒã‚¯ç§»å‹•
            temp_path = checkpoint_path.with_suffix('.tmp')
            torch.save(checkpoint_data, temp_path)
            shutil.move(temp_path, checkpoint_path)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self.recovery_metadata['last_checkpoint'] = str(checkpoint_path)
            self.save_recovery_metadata()
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆï¼ˆæœ€è‰¯æ€§èƒ½æ™‚ï¼‰
            if checkpoint_type == "best":
                backup_path = self.backup_dir / f"best_{checkpoint_name}"
                shutil.copy2(checkpoint_path, backup_path)
                
            print(f"ğŸ’¾ Checkpoint saved: {checkpoint_name}")
            print(f"   Epoch: {epoch}, Accuracy: {best_accuracy:.2f}%, Loss: {loss:.4f}")
            
            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤ï¼ˆæœ€æ–°5å€‹ä¿æŒï¼‰
            self.cleanup_old_checkpoints()
            
            return str(checkpoint_path)
            
        except Exception as e:
            print(f"âŒ Checkpoint save failed: {e}")
            traceback.print_exc()
            return None
            
    def load_checkpoint(self, checkpoint_path: Optional[str] = None) -> Optional[Dict[str, Any]]:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        
        if checkpoint_path is None:
            # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè‡ªå‹•æ¤œç´¢
            checkpoint_path = self.find_latest_checkpoint()
            
        if not checkpoint_path or not Path(checkpoint_path).exists():
            print("ğŸ“ No checkpoint found for recovery")
            return None
            
        try:
            print(f"ğŸ”„ Loading checkpoint: {Path(checkpoint_path).name}")
            
            # CUDAåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
            map_location = 'cuda' if torch.cuda.is_available() else 'cpu'
            checkpoint_data = torch.load(checkpoint_path, map_location=map_location)
            
            # äº’æ›æ€§ç¢ºèª
            if checkpoint_data.get('cuda_device') != self.recovery_metadata['cuda_device']:
                print(f"âš ï¸ Device mismatch: {checkpoint_data.get('cuda_device')} -> {self.recovery_metadata['cuda_device']}")
                
            # ãƒªã‚«ãƒãƒªãƒ¼ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
            self.recovery_metadata['recovery_count'] += 1
            
            print(f"âœ… Checkpoint loaded successfully")
            print(f"   Session: {checkpoint_data.get('session_id', 'Unknown')}")
            print(f"   Epoch: {checkpoint_data.get('epoch', 0)}")
            print(f"   Best Accuracy: {checkpoint_data.get('best_accuracy', 0):.2f}%")
            print(f"   Recovery Count: {self.recovery_metadata['recovery_count']}")
            
            return checkpoint_data
            
        except Exception as e:
            print(f"âŒ Checkpoint load failed: {e}")
            traceback.print_exc()
            return None
            
    def find_latest_checkpoint(self) -> Optional[str]:
        """æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œç´¢"""
        checkpoint_files = list(self.checkpoint_dir.glob("nkat_checkpoint_*.pth"))
        
        if not checkpoint_files:
            return None
            
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        latest_file = max(checkpoint_files, key=lambda p: p.stat().st_mtime)
        return str(latest_file)
        
    def force_checkpoint(self, reason: str = "manual"):
        """å¼·åˆ¶ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆç·Šæ€¥æ™‚ç”¨ï¼‰"""
        print(f"ğŸš¨ Force checkpoint: {reason}")
        
        # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ä¿å­˜
        emergency_data = {
            'timestamp': datetime.now().isoformat(),
            'reason': reason,
            'session_id': self.recovery_metadata['session_id'],
            'recovery_count': self.recovery_metadata['recovery_count'],
            'cuda_memory_allocated': torch.cuda.memory_allocated() if torch.cuda.is_available() else 0,
            'system_memory_percent': psutil.virtual_memory().percent
        }
        
        emergency_file = self.checkpoint_dir / f"emergency_{reason}_{datetime.now().strftime('%H%M%S')}.json"
        with open(emergency_file, 'w') as f:
            json.dump(emergency_data, f, indent=2)
            
        print(f"ğŸ†˜ Emergency state saved: {emergency_file.name}")
        
    def save_recovery_metadata(self):
        """ãƒªã‚«ãƒãƒªãƒ¼ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        metadata_file = self.checkpoint_dir / "recovery_metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(self.recovery_metadata, f, indent=2)
            
    def auto_checkpoint_wrapper(self, 
                               save_func,
                               model: torch.nn.Module,
                               optimizer: torch.optim.Optimizer,
                               scheduler: Optional[Any],
                               epoch: int,
                               best_accuracy: float,
                               loss: float,
                               metrics: Dict[str, Any]):
        """è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå®Ÿè¡Œãƒ©ãƒƒãƒ‘ãƒ¼"""
        
        # å‰å›ä¿å­˜ã‹ã‚‰ã®çµŒéæ™‚é–“ãƒã‚§ãƒƒã‚¯
        current_time = time.time()
        last_save_time = getattr(self, '_last_auto_save', 0)
        
        if current_time - last_save_time >= self.auto_save_interval:
            checkpoint_path = save_func(
                model, optimizer, scheduler, epoch, 
                best_accuracy, loss, metrics, "auto"
            )
            self._last_auto_save = current_time
            return checkpoint_path
        return None
        
    def recovery_training_loop(self, 
                              model: torch.nn.Module,
                              optimizer: torch.optim.Optimizer,
                              scheduler: Optional[Any],
                              train_loader,
                              val_loader,
                              epochs: int,
                              device: torch.device):
        """ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—"""
        
        print("ğŸš€ Starting recovery-enabled training loop")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©å…ƒè©¦è¡Œ
        checkpoint_data = self.load_checkpoint()
        start_epoch = 0
        best_accuracy = 0
        
        if checkpoint_data:
            model.load_state_dict(checkpoint_data['model_state_dict'])
            optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])
            if scheduler and checkpoint_data.get('scheduler_state_dict'):
                scheduler.load_state_dict(checkpoint_data['scheduler_state_dict'])
            start_epoch = checkpoint_data['epoch'] + 1
            best_accuracy = checkpoint_data['best_accuracy']
            
            print(f"ğŸ”„ Resumed from epoch {start_epoch}")
            
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        for epoch in range(start_epoch, epochs):
            print(f"\n{'='*50}")
            print(f"Epoch {epoch+1}/{epochs} (Recovery Count: {self.recovery_metadata['recovery_count']})")
            print(f"{'='*50}")
            
            try:
                # Training phase
                model.train()
                train_loss = 0
                correct = 0
                total = 0
                
                progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")
                
                for batch_idx, (data, target) in enumerate(progress_bar):
                    data, target = data.to(device), target.to(device)
                    
                    optimizer.zero_grad()
                    output = model(data)
                    loss = torch.nn.functional.cross_entropy(output, target)
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                    pred = output.argmax(dim=1)
                    correct += pred.eq(target).sum().item()
                    total += target.size(0)
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
                    accuracy = 100. * correct / total
                    progress_bar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{accuracy:.2f}%'
                    })
                    
                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
                    if torch.cuda.is_available() and batch_idx % 100 == 0:
                        memory_used = torch.cuda.memory_allocated() / 1024**3
                        if memory_used > 10:  # 10GBè¶…éæ™‚è­¦å‘Š
                            print(f"\nâš ï¸ High GPU memory usage: {memory_used:.1f}GB")
                            
                # Validation phase
                model.eval()
                val_loss = 0
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for data, target in tqdm(val_loader, desc="Validation"):
                        data, target = data.to(device), target.to(device)
                        output = model(data)
                        val_loss += torch.nn.functional.cross_entropy(output, target).item()
                        pred = output.argmax(dim=1)
                        val_correct += pred.eq(target).sum().item()
                        val_total += target.size(0)
                        
                train_acc = 100. * correct / total
                val_acc = 100. * val_correct / val_total
                avg_train_loss = train_loss / len(train_loader)
                avg_val_loss = val_loss / len(val_loader)
                
                print(f"\nEpoch {epoch+1} Results:")
                print(f"  Train: Loss={avg_train_loss:.4f}, Acc={train_acc:.2f}%")
                print(f"  Val:   Loss={avg_val_loss:.4f}, Acc={val_acc:.2f}%")
                
                # Best modelæ›´æ–°
                if val_acc > best_accuracy:
                    best_accuracy = val_acc
                    checkpoint_type = "best"
                else:
                    checkpoint_type = "regular"
                    
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                metrics = {
                    'train_accuracy': train_acc,
                    'val_accuracy': val_acc,
                    'train_loss': avg_train_loss,
                    'val_loss': avg_val_loss
                }
                
                self.save_checkpoint(
                    model, optimizer, scheduler, epoch,
                    best_accuracy, avg_val_loss, metrics, checkpoint_type
                )
                
                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—
                if scheduler:
                    scheduler.step()
                    
            except Exception as e:
                print(f"\nâŒ Training error at epoch {epoch}: {e}")
                traceback.print_exc()
                
                # ç·Šæ€¥ä¿å­˜
                self.force_checkpoint(f"training_error_epoch_{epoch}")
                
                # CUDA ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                print("ğŸ”„ Attempting to continue training...")
                continue
                
        print(f"\nâœ… Training completed! Best accuracy: {best_accuracy:.2f}%")
        return best_accuracy
        
    def cleanup_old_checkpoints(self, keep_count: int = 5):
        """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤"""
        checkpoint_files = list(self.checkpoint_dir.glob("nkat_checkpoint_auto_*.pth"))
        
        if len(checkpoint_files) > keep_count:
            # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‰Šé™¤
            old_files = sorted(checkpoint_files, key=lambda p: p.stat().st_mtime)[:-keep_count]
            for old_file in old_files:
                old_file.unlink()
                print(f"ğŸ—‘ï¸ Removed old checkpoint: {old_file.name}")
                
    def generate_recovery_report(self) -> str:
        """ãƒªã‚«ãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        checkpoint_files = list(self.checkpoint_dir.glob("nkat_checkpoint_*.pth"))
        emergency_files = list(self.checkpoint_dir.glob("emergency_*.json"))
        
        report = f"""
ğŸ”‹ NKAT Power Recovery System Report
{'='*60}

ğŸ“Š Session Information:
   Session ID: {self.recovery_metadata['session_id']}
   Start Time: {self.recovery_metadata['start_time']}
   Recovery Count: {self.recovery_metadata['recovery_count']}
   CUDA Device: {self.recovery_metadata['cuda_device']}

ğŸ’¾ Checkpoint Status:
   Total Checkpoints: {len(checkpoint_files)}
   Latest Checkpoint: {Path(self.recovery_metadata.get('last_checkpoint', 'None')).name if self.recovery_metadata.get('last_checkpoint') else 'None'}
   Emergency Saves: {len(emergency_files)}

ğŸ”§ System Status:
   CUDA Available: {torch.cuda.is_available()}
   GPU Memory: {torch.cuda.memory_allocated() / 1024**3:.1f}GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB
   CPU Memory: {psutil.virtual_memory().percent:.1f}%

ğŸ“ Storage:
   Checkpoint Dir: {self.checkpoint_dir}
   Backup Dir: {self.backup_dir}
   Auto Save Interval: {self.auto_save_interval}s

{'='*60}
        """
        
        print(report)
        return report

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    recovery_system = NKATPowerRecoverySystem()
    recovery_system.generate_recovery_report() 